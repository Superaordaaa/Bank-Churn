# -*- coding: utf-8 -*-
"""Final Project - Boba Warriors.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K5c_Myn1oIXUH56LszS0FSlH7vki8_CN

Nomor 1
"""

# library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Nomor 1
from google.colab import files
file = files.upload()

!pip install --upgrade --force-reinstall xlrd

import io
df = pd.read_csv(io.StringIO(file['Churn_Modelling.csv'].decode('utf-8')))

df.head()

df.info()

df.shape

df.duplicated().any()

df[df.duplicated(keep=False)==True]

#melihat apakah terdapat kolom kosong atau tidak pada dataset
df.isna().sum()

#melihat tipe data pada dataset
df.dtypes

#menghapus kolom yang tidak dibutuhkan pada data set
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1)

df.head()

df.describe()

numericals = ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary']
categoricals = ['Geography','Gender', 'HasCrCard', 'IsActiveMember', 'Exited']

df[categoricals].describe()

"""pertanyaan
1. Apakah ada kolom dengan tipe data kurang sesuai, atau nama kolom dan isinya
kurang sesuai?
2. Apakah ada kolom yang memiliki nilai kosong? Jika ada, apa saja?
3. Apakah ada kolom yang memiliki nilai summary agak aneh?
(min/mean/median/max/unique/top/freq) 


jawaban 


1.   Semua tipe data yang tersedia pada dataset sudah sesuai 
2.   Pada dataset tidak ada nilai kosong
3.   Dari data set yang tersedia tidak terdapat nilai aneh pada nilai min/mean/median/max/unique/top/freq

Nomor 2
"""

#Nomor 2

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

list(enumerate(numericals))

dfi = pd.read_csv("Churn_Modelling.csv")

plt.figure(figsize = (10,20))
for i in enumerate(numericals):
    plt.subplot(3,4,i[0]+1)
    sns.kdeplot(i[1], data=dfi)
    plt.xticks(rotation=45)

"""Berdasarkan hasil distribusi diatas: 
- Features EstimatedSalary, CreditScore, dan tenure sudah termasuk kedalam distribusi normal, 
- features age terdistribusi positif karena terdapat ketimpangan usia yaitu dengan nilai usia tertinggi 92 tahun sedangkan rata-rata usia terletak di 32 tahun, untuk menanggulangi hal tersebut di data pre-processing dapat menggunakan normalisasi data
- Features Numofproducts dapat termasuk kedalam categorical yaitu dengan nilai tertinggi yaitu 1 dan 2 yang artinya jumlah produk yang dibeli pelanggan terbanyak yaitu membeli 1 atau 2 produk
- Features balance juga terdistribusi bimodal positif skew karena terdapat juga ketimpangan data dengan nilai 0 mendominasi, untuk menyelesaikan masalah tersebut pada tahapan data pre-processing dapat dilakukannya normalisasi data, standartlisasi data, atau Log transformation 
- Dilihat dari data distribusi tersebut, mayoritas features datanya homogen/runtun
"""

plt.figure(figsize = (10,20))
for i in enumerate(numericals):
    plt.subplot(3,4,i[0]+1)
    sns.boxplot(i[1], data=dfi)

"""- Berdasarkan hasil boxplot diatas dapat disimpulkan bahwa terdapat features yang terdapat outlier yaitu pada features creaditscore, age, dan numofproduct karena terdapat data yang ekstrem kebawah atau keatas untuk menyelesaikan masalah tersebut pada tahapan data pre-processing dapat dilakukannya menghapus oulier berdasarkan IQR atau Z-score
- Features yang akan menjadi perhatian khusus yakni pada features CrediteScore dan Age mengingat jarak outliernya cukup jauh dari rata-ratanya.
"""

plt.figure(figsize = (10,20))
for i in enumerate(numericals):
    plt.subplot(3,4,i[0]+1)
    sns.violinplot(i[1], data=df)

"""Berdasarkan hasil violin plot diatas dapat dilihat apa yang dideskripsikan pada boxplot maupun displot sesuai.

> Indented block


"""

plt.figure(figsize = (15,10))
for i in range(0, len(categoricals)):
   plt.subplot(1, len(categoricals), i+1)
   sns.countplot(x=df[categoricals[i]], color='green')
   plt.tight_layout()

"""Berdasarkan hasil countplot 
- Kategori diatas menjelaskan bahwa terdapat ketimpangan jumlah pengguna berdasarkan features geography yaitu jumlah pengguna France lebih banyak dibandingkan spain dan germany.
- Features HasCrCard yang menunjukkan bahwa nasabah yang memiliki kartu kredit lebih banyak dibandingkan dengan yang tidak memiliki kartu kredit.
- Pada features exited, dapat disimpulkan bahwa nasabah pada bank tersebut mayoritas tidak churn dibandingkan dengan yang churn, untuk menyelesaikan masalah tersebut pada tahapan data pre-processing dapat dilakukannya Class imbalance.
- Features gender dan IsActiveMember memiliki nilai yang tidak begitu timpang

Nomor 3

Lakukan multivariate analysis (seperti correlation heatmap dan category plots, sesuai yang
diajarkan di kelas). Tuliskan hasil observasinya, seperti:<br>
A. Bagaimana korelasi antara masing-masing feature dan label. Kira-kira feature mana 
saja yang paling relevan dan harus dipertahankan?
B. Bagaimana korelasi antar-feature, apakah ada pola yang menarik? Ap
"""

#Nomor 3
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dfx = pd.read_csv("Churn_Modelling.csv")

dfx.corr()

plt.figure(figsize=(8, 8))
sns.heatmap(dfx.corr(), cmap='Blues', annot=True, fmt='.2f')

dfx.info()

# pengelompokan kolom berdasarkan jenisnya
cats = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'Exited']
nums = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']

fig = plt.figure(figsize=(15, 15))
for i in range(0, len(nums)): # untuk setiap kolom numerik
    ax = fig.add_subplot(3, 3, i+1) # kita set posisi catplot/stripplotnya di layout
    sns.stripplot(ax=ax, data=dfx, x='Geography', y=nums[i]) # gambar catplot/stripplotnya
    plt.tight_layout()

fig = plt.figure(figsize=(15, 15))
for i in range(0, len(nums)): # untuk setiap kolom numerik
    ax = fig.add_subplot(3, 3, i+1) # kita set posisi catplot/stripplotnya di layout
    sns.stripplot(ax=ax, data=dfx, x='Gender', y=nums[i]) # gambar catplot/stripplotnya
    plt.tight_layout()

fig = plt.figure(figsize=(15, 15))
for i in range(0, len(nums)): # untuk setiap kolom numerik
    ax = fig.add_subplot(3, 3, i+1) # kita set posisi catplot/stripplotnya di layout
    sns.stripplot(ax=ax, data=dfx, x='HasCrCard', y=nums[i]) # gambar catplot/stripplotnya
    plt.tight_layout()

fig = plt.figure(figsize=(15, 15))
for i in range(0, len(nums)): # untuk setiap kolom numerik
    ax = fig.add_subplot(3, 3, i+1) # kita set posisi catplot/stripplotnya di layout
    sns.stripplot(ax=ax, data=dfx, x='IsActiveMember', y=nums[i]) # gambar catplot/stripplotnya
    plt.tight_layout()

fig = plt.figure(figsize=(15, 15))
for i in range(0, len(nums)): # untuk setiap kolom numerik
    ax = fig.add_subplot(3, 3, i+1) # kita set posisi catplot/stripplotnya di layout
    sns.stripplot(ax=ax, data=dfx, x='Exited', y=nums[i]) # gambar catplot/stripplotnya
    plt.tight_layout()

dfx.info()

sns.kdeplot(data=dfx, x="CreditScore", hue="Exited")

sns.kdeplot(data=dfx, x="Age", hue="Exited")

sns.kdeplot(data=dfx, x="Tenure", hue="Exited")

sns.kdeplot(data=dfx, x="Balance", hue="Exited")

sns.kdeplot(data=dfx, x="NumOfProducts", hue="Exited")

sns.kdeplot(data=dfx, x="EstimatedSalary", hue="Exited")

fig, axarr = plt.subplots(2, 2, figsize=(20, 12))
sns.countplot(x='Geography', hue = 'Exited',data = dfx, ax=axarr[0][0])
sns.countplot(x='Gender', hue = 'Exited',data = dfx, ax=axarr[0][1])
sns.countplot(x='HasCrCard', hue = 'Exited',data = dfx, ax=axarr[1][0])
sns.countplot(x='IsActiveMember', hue = 'Exited',data = dfx, ax=axarr[1][1])

"""setelah dilakukan multivariate analisis tidak ditemukan hubungan yang signifikan pada mayoritas 
perbandingan feature. dapat dilihat dari heatmap, data yang nilai pengaruhnya paling besar adalah age(umur)
dengan exited(tutup rekening) yang bernilai 0.29 sedangkan rata rata korelasinya tidak lebih dari 0.09<br>di luar terhadap target, nilai korelasi Balance dan Number of Products juga lebih besar dibanding yang lain dan sifatnya berlawanan, namun tidak begitu besar signifikannya dengan nilai -0.30

Nomor 4

Selain EDA, lakukan juga beberapa analisis dan visualisasi untuk menemukan suatu
business insight. Tuliskan minimal 3 insight, dan berdasarkan insight tersebut jelaskan
rekomendasinya untuk bisnis.
"""

df = pd.read_csv("Churn_Modelling.csv")

plt.figure(figsize=(10,8))
sns.countplot(x='Geography', hue='Exited', data=df)
plt.axhline(y=800, linestyle='--', color='red')

plt.title('Jumlah Nasabah Berdasarkan Kewarganegaraan', fontsize=18, fontweight='bold')
plt.xlabel('Total Nasabah ', fontsize=14)

"""Dari visualisasi di atas dapat disimpulkan bahwa negara france dan Germany memiliki tingkat Churn yang sama dan negara Spain adalah negara dengan tingkat churn paling rendah, rekomendasi bisnis yang mungkin dapat di berikan adalah memberikan perlakuan khusu seperti customer engagement atau menawakan keuntungan jakngka panjang seperti membership program kepada France dan Germany"""

df['tenure_group'] = np.where(df['Tenure'] > 7, 'High', 
                                             np.where(df['Tenure'] > 4, 'Mid', 'Low'))
df_group = df.groupby(['tenure_group'])['CustomerId'].nunique().reset_index(name='unique_customer')
df_group.sort_values('unique_customer', ascending=False)

plt.figure(figsize = (13,8))
sns.barplot(x='tenure_group',
            y='unique_customer',
            data=df_group)

plt.ylabel('Total tenure', fontsize = 13, labelpad = 20)
plt.xlabel('unique_customer', fontsize = 13, labelpad = 20)
plt.title('Number of Customer by Tenure Group',
          fontweight='bold',
          fontsize = 20)

"""Dari visualisasi di atas dapat disimpulkan bahwa nasabah dengan masa tenure low lebih banyak atau kebanyakan nasabah bank mimiliki tenure kurang dari 4 tahun, rekomendasi bisnis yang mungkin di berikan adlah memberikan riward kepada nasabah dengan masa tenure tertentu agar supaya nasabah tetap bertahan dan tidak jdi churn."""

fig = plt.figure(figsize=(15, 15))
ax = fig.add_subplot(3, 3, i+1) # kita set posisi catplot/stripplotnya di layout
sns.stripplot(ax=ax, data=df, x='Exited', y='CreditScore') # gambar catplot/stripplotnya
plt.tight_layout()

"""Dari visualisasi di atas dapat disimpulkan bahwa nasabah yang memiliki credit score dibawah 400 akan cenderung melakukan churn, oleh karena itu kita dapat memperingati kepada perusahaan jika ada nasabah yang akan hit credit scorenya menjadi 400 kebawah, maka potensi churnnya akan lebih besar. Sehingga pihak bank dapat melakukan tindakan persuasi kepada nasabah tersebut seperti memberikan promo menarik."""



"""# Data Cleansing

**Handle Mising Value**
"""

df.info()

df.isna().sum()

"""**Handle Duplicates Data**"""

df.duplicated().sum()

"""**Handle Outliers**"""

from scipy import stats

dfz = pd.read_csv("Churn_Modelling.csv")

print(f'Jumlah baris sebelum memfilter outlier: {len(dfz)}')

filtered_entries = np.array([True] * len(dfz))

for col in ['Age', 'NumOfProducts', 'CreditScore']:
    zscore = abs(stats.zscore(dfz[col])) # hitung absolute z-scorenya
    filtered_entries = (zscore < 3) & filtered_entries # keep yang kurang dari 3 absolute z-scorenya
    
dfz = dfz[filtered_entries] # filter, cuma ambil yang z-scorenya dibawah 3

print(f'Jumlah baris setelah memfilter outlier: {len(dfz)}')

sns.kdeplot(dfz['Age'])

sns.kdeplot(dfz['CreditScore'])

"""### Start Here"""

df1 = pd.read_csv("Churn_Modelling.csv")

#Add feature Credit Score that's under 400
df1['CSunder400'] = df1['CreditScore'].apply(lambda x: 1 if x < 400 else 0)

# from sklearn.model_selection import train_test_split

# train, test = train_test_split(df1, test_size=0.3)

print(f'Jumlah baris sebelum memfilter outlier: {len(df1)}')

filtered_entries = np.array([True] * len(df1))

for col in ['CreditScore','Age','Tenure','Balance','EstimatedSalary']:

      Q1 = df1[col].quantile(0.25)
      Q3 = df1[col].quantile(0.75)
      IQR = Q3 - Q1
      low_limit = Q1 - (IQR * 1.5)
      high_limit = Q3 + (IQR * 1.5)

      filtered_entries = ((df1[col] >= low_limit) & (df1[col] <= high_limit)) & filtered_entries

df1 = df1[filtered_entries]

print(f'Jumlah baris setelah memfilter outlier: {len(df1)}')

"""**Feature Transformation**"""

# X = df1.drop(columns=['Exited'])
# y = df1['Exited']

# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# numericals = ['CreditScore','Age','Tenure','Balance','EstimatedSalary']
# categoricals = ['Geography','Gender', 'HasCrCard', 'IsActiveMember', 'Exited', 'NumOfProducts']

# list(enumerate(numericals))
# plt.figure(figsize = (20,20))
# for i in enumerate(numericals):
#     plt.subplot(3,4,i[0]+1)
#     sns.kdeplot(i[1], data=X_train)
#     plt.xticks(rotation=45)

"""## **Feature Encoding**"""

# label
mapping_jenis_kelamin = {
    'Male' : 0,
    'Female' : 1
}

df1['jenis_kelamin'] = df1['Gender'].map(mapping_jenis_kelamin)

# OHE
for cat in ['Geography']:
    onehots = pd.get_dummies(df1[cat], prefix=cat)
    df1 = df1.join(onehots)

# transformasi data dengan standarisasi
# karena semuah fitur righ-skeww ada di vaibel x maka dilakukan standarisasi pada vaibel x
# from sklearn.preprocessing import StandardScaler
# ss = StandardScaler()

# numerical_features = X.columns.to_list()
# for n in numerical_features:
#   scaler = ss.fit(X_train[[n]])
#   X_train[n] = scaler.transform(X_train[[n]])
#   X_test[n] = scaler.transform(X_test[[n]])

from sklearn.preprocessing import MinMaxScaler, StandardScaler

# sebelum dilakukan transformasi dilakukan split terlebih dahulu
# split yang dilakukan langsung memisah kan featur dan target

X = df1.drop(columns=['RowNumber','CustomerId', 'Surname','Gender','Geography','Exited'])
y = df1[['Exited']]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaler = MinMaxScaler()
X_train_scales = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train['Age_std'] = StandardScaler().fit_transform(X_train['Age'].values.reshape(len(X_train), 1))
X_train['Balance_std'] = StandardScaler().fit_transform(X_train['Balance'].values.reshape(len(X_train), 1))
X_train['CreditScore_norm'] = MinMaxScaler().fit_transform(X_train['CreditScore'].values.reshape(len(X_train), 1))
X_train['Age_norm'] = MinMaxScaler().fit_transform(X_train['Age_std'].values.reshape(len(X_train), 1))
X_train['Balance_norm'] = MinMaxScaler().fit_transform(X_train['Balance_std'].values.reshape(len(X_train), 1))
X_train['Tenure_norm'] = MinMaxScaler().fit_transform(X_train['Tenure'].values.reshape(len(X_train), 1))
X_train['EstimatedSalary_norm'] = MinMaxScaler().fit_transform(X_train['EstimatedSalary'].values.reshape(len(X_train), 1))

MinMaxScaler().fit(X_train[['CreditScore']])
X_test['CreditScore_norm'] = MinMaxScaler().transform(X_test['CreditScore'].values.reshape(len(X_test), 1))

numericals1 = ['CreditScore_norm','Age_norm','Tenure_norm','Balance_norm','EstimatedSalary_norm',]

X_test['Age_std'] = StandardScaler().transform(X_test['Age'].values.reshape(len(X_test), 1))
X_test['Balance_std'] = StandardScaler().transform(X_test['Balance'].values.reshape(len(X_test), 1))

X_test['CreditScore_norm'] = MinMaxScaler().fit_transform(X_test['CreditScore'].values.reshape(len(X_test), 1))
X_test['Age_norm'] = MinMaxScaler().fit_transform(X_test['Age_std'].values.reshape(len(X_test), 1))
X_test['Balance_norm'] = MinMaxScaler().fit_transform(X_test['Balance_std'].values.reshape(len(X_test), 1))
X_test['Tenure_norm'] = MinMaxScaler().fit_transform(X_test['Tenure'].values.reshape(len(X_test), 1))
X_test['EstimatedSalary_norm'] = MinMaxScaler().fit_transform(X_test['EstimatedSalary'].values.reshape(len(X_test), 1))

list(enumerate(numericals1))
plt.figure(figsize = (20,20))
for i in enumerate(numericals1):
    plt.subplot(3,4,i[0]+1)
    sns.kdeplot(i[1], data=X_train)
    plt.xticks(rotation=45)

# drop kolom asli (nilai asli)
X_train = X_train.drop(columns=['RowNumber', 'CustomerId', 'Surname'])

"""##**Feature Imbalance**"""

X_train = X_train.drop(columns=['Geography', 'Gender'])

# pemisahan features vs target
x = X_train[[col for col in X_train.columns if col not in ['Exited']]].values
y = y_train.values

print(x.shape)
print(y.shape)

from imblearn import under_sampling, over_sampling
x_over_SMOTE, y_over_SMOTE = over_sampling.SMOTE().fit_resample(x,y)

print(pd.Series(y).value_counts())

print(pd.Series(y_over_SMOTE).value_counts())

x_over_SMOTE

"""1.   setelah dilakukan pengecekan dengan .info() dan isna().sum() tidak di temukan mising value 
2.   setelah dilakukan pengecekan dengan  duplicated().sum() tidak di temukan duplikat data
3. Feature CreditScore, Age, Tenure, Balance, dan Estimated salary dari outlier difilter dengan menggunakan IQR yang hasilnya jumlah baris berkurang menjadi 9626 
4. kolom Age dan Balance memiliki persebaran data right-skewed yang kemudian dilakukan transformasi data menggunakan standarisasi 
5. encoding di lakukan pada kolom Geography dan Gender, pada kolom Geography dilakukan metode one hot encoding dan gender menggunakan metode label encoding
6. Class imbalance telah dilakukan dan tiap kelas memiliki 7677 data

FEATURE ENGINEERING

a. Feature Selection (membuang feature yang kurang relevan atau redundan)

b. Feature Extraction (membuat feature baru dan feature yang sudah ada)

c. Tuliskan minimal 4 feature tambahan (selain yang sudah tersedia di dataset) yang mungkin akan sangat membantu membuat performansi model semakin bagus
```
"""

train.head()

plt.figure(figsize=(15, 15))
sns.heatmap(X_train.corr(), cmap='Blues', annot=True, fmt='.2f')

X_train = X_train.drop(columns=['CreditScore', 'Age', 'Age_std', 'Tenure', 'Balance', 'Balance_std', 'EstimatedSalary'])

plt.figure(figsize=(15, 15))
sns.heatmap(X_train.corr(), cmap='Blues', annot=True, fmt='.2f')

dfz['BalanceSalaryRatio'] = dfz.Balance/dfz.EstimatedSalary
sns.boxplot(y='BalanceSalaryRatio',x = 'Exited', hue = 'Exited',data = dfz, orient='v')
plt.ylim(-1, 5)

df_x['BalanceSalaryRatio'] = dfz['BalanceSalaryRatio']

"""Berdasarkan HeatMap di atas EstimatedSalary memiliki pengaruh yang kecil terhadap peluang pelanggan untuk Churn. Namun setelah adanya penambahan feature baru rasio Balance dan Estimated Salary (Perhatikan boxplot BalanceSalaryRatio) menunjukkan bahwa nasabah dengan rasio Estimated Salary yang lebih tinggi akan lebih banyak churn. Hal tersebut dapat menjadi perhatian bank karena khawatir berdampak pada sumber modal pinjaman."""

df_x["CreditsScore"] = pd.qcut(df_x['CreditScore'], 6, labels = [1, 2, 3, 4, 5, 6])
df_x.head()

#3Membuat label pada umur untuk mengetahui di umur berapa yang akan melakukan churn 
dfz["AgeScore"] = pd.qcut(dfz['Age'], 5, labels = [1, 2, 3, 4, 5])

df_x["AgeScore"] = dfz["AgeScore"]

df_x["TenureScore"] = pd.qcut(df_x['Tenure'], 5, labels = [1, 2, 3, 4, 5])
df_x.head()

df_x['CSunder400'] = df_x['CreditScore'].apply(lambda x: 1 if x < 400 else 0)

df_x

"""# Modeling

## Split Data Train & Test
"""

train.info()

"""##### Test data transform"""

# Delete This 
#Normaliasi (scaling)
#train['CreditScore_norm'] = MinMaxScaler().fit_transform(train['CreditScore'].values.reshape(len(train), 1))
MinMaxScaler().fit(X_train[['CreditScore']])
X_test['CreditScore_norm'] = MinMaxScaler().transform(X_test['CreditScore'].values.reshape(len(X_test), 1))

#train['Age_norm'] = MinMaxScaler().fit_transform(train['Age_std'].values.reshape(len(train), 1))
#train['Balance_norm'] = MinMaxScaler().fit_transform(train['Balance_std'].values.reshape(len(train), 1))
#train['Tenure_norm'] = MinMaxScaler().fit_transform(train['Tenure'].values.reshape(len(train), 1))
#train['EstimatedSalary_norm'] = MinMaxScaler().fit_transform(train['EstimatedSalary'].values.reshape(len(train), 1))

X_test = X_test.drop(columns=['RowNumber', 'CustomerId', 'Surname'])

X_test['jenis_kelamin'] = X_test['Gender'].map(mapping_jenis_kelamin)

# OHE
for cat in ['Geography']:
    onehots = pd.get_dummies(X_test[cat], prefix=cat)
    X_test = X_test.join(onehots)

X_test = X_test.drop(columns=['Geography', 'Gender'])

X_test = X_test.drop(columns=['CreditScore', 'Age', 'Age_std', 'Tenure', 'Balance', 'Balance_std', 'EstimatedSalary'])

"""### Next"""

import warnings
warnings.filterwarnings('ignore')

X_train = train.drop(columns='Exited')
y_train = train['Exited']
X_test = test.drop(columns='Exited')
y_test = test['Exited']

X = pd.concat([X_test, X_train])
y = pd.concat([y_test, y_train])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_validate

def eval_classification(model):
    y_pred = model.predict(X_test)
    y_pred_train = model.predict(X_train)
    y_pred_proba = model.predict_proba(X_test)
    y_pred_proba_train = model.predict_proba(X_train)
    
    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, y_pred))
    print("Precision (Test Set): %.2f" % precision_score(y_test, y_pred))
    print("Recall (Test Set): %.2f" % recall_score(y_test, y_pred))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, y_pred))
    
    print("roc_auc (test-proba): %.2f" % roc_auc_score(y_test, y_pred_proba[:, 1]))
    print("roc_auc (train-proba): %.2f" % roc_auc_score(y_train, y_pred_proba_train[:, 1]))

    score = cross_validate(model, X, y, cv=5, scoring='roc_auc', return_train_score=True)
    print('roc_auc (crossval train): '+ str(score['train_score'].mean()))
    print('roc_auc (crossval test): '+ str(score['test_score'].mean()))

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model):
    print(model.best_estimator_.get_params())

import numpy as np
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

def draw_learning_curve(param_values):
    train_scores = []
    test_scores = []

    for c in param_values:
        model = LogisticRegression(penalty='l2', C=c)
        model.fit(X_train, y_train)

        # eval on train
        y_pred_train_proba = model.predict_proba(X_train)
        train_auc = roc_auc_score(y_train, y_pred_train_proba[:,1])
        train_scores.append(train_auc)

        # eval on test
        y_pred_proba = model.predict_proba(X_test)
        test_auc = roc_auc_score(y_test, y_pred_proba[:,1])
        test_scores.append(test_auc)

        print('param value: ' + str(c) + '; train: ' + str(train_auc) + '; test: '+ str(test_auc))

    plt.plot(param_values, train_scores, label='Train')
    plt.plot(param_values, test_scores, label='Test')
    plt.xlabel('C')
    plt.ylabel('AUC')
    plt.title('Learning Curve - Hyperparameter C - Logistic Regression')
    plt.legend()
    plt.show()

df1.Exited.value_counts(normalize=True)

"""## Modeling

### Logistic Regression

#### Model Fit
"""

#Logistic Regression
from sklearn.linear_model import LogisticRegression # import logistic regression dari sklearn
logreg = LogisticRegression() # inisiasi object dengan nama logreg
logreg.fit(X_train, y_train) # fit model regression dari data train
eval_classification(logreg)

"""#### Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV

penalty = ['l1', 'l2']
C = [float(x) for x in np.linspace(0.0001, 0.05, 100)]
hyperparameters = dict(penalty=penalty, C=C)

logreg = LogisticRegression()
rs = RandomizedSearchCV(logreg, hyperparameters, scoring='roc_auc', random_state=1, cv=5, n_iter=50)
rs.fit(X_train, y_train)
eval_classification(rs)

"""#### Check the best hyperparameter after tuning"""

show_best_hyperparameter(rs)

"""#### Feature Importance"""

X.columns

rs.best_estimator_.coef_

"""#### Learning Curve"""

# learning curve
import numpy as np
param_values = [float(x) for x in np.linspace(0.0001, 0.05, 100)]
draw_learning_curve(param_values)

"""### KNN

#### Model Fit
"""

# knn
from sklearn.neighbors import KNeighborsClassifier # import knn dari sklearn
knn = KNeighborsClassifier() # inisiasi object dengan nama knn
knn.fit(X_train, y_train) # fit model KNN dari data train
eval_classification(knn)

"""#### Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV

n_neighbors = list(range(1,30))
p=[1,2]
algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']
hyperparameters = dict(n_neighbors=n_neighbors, p=p, algorithm=algorithm)

knn.fit(X_train, y_train)
rs = RandomizedSearchCV(knn, hyperparameters, scoring='roc_auc', random_state=1, cv=5)
rs.fit(X_train, y_train)
eval_classification(rs)

"""#### Learning Curve"""

# Analyzing Learning Curve
import numpy as np
from matplotlib import pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

def draw_learning_curve(param_values):
    train_scores = []
    test_scores = []

    for i in param_values:
        model = KNeighborsClassifier(n_neighbors=i)
        model.fit(X_train, y_train)

        # eval on train
        y_pred_train_proba = model.predict_proba(X_train)
        train_auc = roc_auc_score(y_train, y_pred_train_proba[:,1])
        train_scores.append(train_auc)

        # eval on test
        y_pred_proba = model.predict_proba(X_test)
        test_auc = roc_auc_score(y_test, y_pred_proba[:,1])
        test_scores.append(test_auc)

        print('param value: ' + str(i) + '; train: ' + str(train_auc) + '; test: '+ str(test_auc))

    plt.plot(param_values, train_scores, label='Train')
    plt.plot(param_values, test_scores, label='Test')
    plt.xlabel('k')
    plt.ylabel('AUC')
    plt.title('Learning Curve')
    plt.legend()
    plt.show()

param_values = [int(x) for x in np.linspace(1, 100, 100)]
draw_learning_curve(param_values)

"""### Decision Tree

#### Model Fit
"""

# decision tree
from sklearn.tree import DecisionTreeClassifier # import decision tree dari sklearn
dt = DecisionTreeClassifier() # inisiasi object dengan nama dt
dt.fit(X_train, y_train) # fit model decision tree dari data train
eval_classification(dt)

"""#### Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy.stats import uniform
import numpy as np

# List of hyperparameter
max_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree
min_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node
min_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node
max_features = ['auto', 'sqrt'] # Number of features to consider at every split
criterion = ['gini','entropy']
splitter = ['best','random']

hyperparameters = dict(max_depth=max_depth, 
                       min_samples_split=min_samples_split, 
                       min_samples_leaf=min_samples_leaf,
                       max_features=max_features,
                       criterion=criterion,
                       splitter=splitter
                      )

# Inisialisasi Model
dt = DecisionTreeClassifier(random_state=42)
model = GridSearchCV(dt, hyperparameters, cv=5, scoring='precision')
model.fit(X_train, y_train)

# Predict & Evaluation
y_pred = model.predict(X_test)#Check performa dari model
eval_classification(model)

"""#### Feature Importance"""

show_feature_importance(model.best_estimator_)

"""# Ini baru"""

# l# library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# load dataset
df = pd.read_csv("Churn_Modelling.csv")
df.sample(6)

import warnings
warnings.filterwarnings('ignore')

"""# Data cleansing

**Handle Mising Value**
"""

df.info()

df.isna().sum()

"""**Handle Duplicates Data**"""

df.duplicated().sum()

"""**Handle Outliers**"""

# menghilangkan outliers dengan IQR

from scipy import stats
print(f'Jumlah baris sebelum memfilter outlier: {len(df)}')

filtered_entries = np.array([True] * len(df))

for col in ['CreditScore','Age','Tenure','Balance','EstimatedSalary']:

      Q1 = df[col].quantile(0.25)
      Q3 = df[col].quantile(0.75)
      IQR = Q3 - Q1
      low_limit = Q1 - (IQR * 1.5)
      high_limit = Q3 + (IQR * 1.5)

      filtered_entries = ((df[col] >= low_limit) & (df[col] <= high_limit)) & filtered_entries

df = df[filtered_entries]

print(f'Jumlah baris setelah memfilter outlier: {len(df)}')

"""**Feature Encoding**"""

# label
mapping_jenis_kelamin = {
    'Male' : 0,
    'Female' : 1
}

df['jenis_kelamin'] = df['Gender'].map(mapping_jenis_kelamin)

# OHE
for cat in ['Geography']:
    onehots = pd.get_dummies(df[cat], prefix=cat)
    df = df.join(onehots)

"""**Feature Transformation**"""

# sebelum dilakukan transformasi dilakukan split terlebih dahulu
# split yang dilakukan langsung memisah kan featur dan target

X = df.drop(columns=['RowNumber','CustomerId', 'Surname','Gender','Geography','Exited' ])
y = df[['Exited']]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# transformasi data dengan standarisasi
# karena semuah fitur righ-skeww ada di vaibel x maka dilakukan standarisasi pada vaibel x
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()

numerical_features = X.columns.to_list()
for n in numerical_features:
  scaler = ss.fit(X_train[[n]])
  X_train[n] = scaler.transform(X_train[[n]])
  X_test[n] = scaler.transform(X_test[[n]])

"""**Feature Imbalance**"""

# featur dan target
x = X_train.values # menggunalan x kecil
y = y_train['Exited'].values # menggunalan y kecil

print(x.shape)
print(y.shape)

from imblearn import under_sampling, over_sampling
X_under, y_under = under_sampling.RandomUnderSampler(0.5).fit_resample(x, y)
X_over, y_over = over_sampling.RandomOverSampler(0.5).fit_resample(x, y)
X_over_SMOTE, y_over_SMOTE = over_sampling.SMOTE(0.5).fit_resample(x, y)

print('Original')
print(pd.Series(y).value_counts())
print('\n')
print('UNDERSAMPLING')
print(pd.Series(y_under).value_counts())
print('\n')
print('OVERSAMPLING')
print(pd.Series(y_over).value_counts())
print('\n')
print('SMOTE')
print(pd.Series(y_over_SMOTE).value_counts())

"""catatatan mengenai data cleansing ada di file asli final project (Final Project - Boba Warriors.ipynb)

# FEATURE ENGINEERING

a. Feature Selection (membuang feature yang kurang relevan atau redundan)

b. Feature Extraction (membuat feature baru dari feature yang sudah ada)

c. Tuliskan minimal 4 feature tambahan (selain yang sudah tersedia di dataset) yang mungkin akan sangat membantu membuat performansi model semakin bagus

**FEATURE ENGINEERING**
  ## ini hanya asumsi beberapa hal tidak di lakukan untuk menghemat waktu##

1.   membuang kolom 'RowNumber','CustomerId', 'Surname', karena tidak dipakai, dan membuang kolom 'Gender','Geography' kareana stetlah di lakukan encodinf kolom ini sudah tidak relevan  ini dilakukan pada saat split data 
2.   proses encoding menghailkan bebrapa fitur baru untuk penyesuaiaan diataranya kolom jenis_kelamin, Geography_France,	Geography_Germany	dan Geography_Spain
3. tidak ada fitur tambhan lagi karena menggunakan fitur yang sudah ada

# Modeling

**A. Split Data Train & Test**
"""

# split sudah dilakukan pada saat transformasi data
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""**B.Modeling**"""

#Logistic Regression

from sklearn.linear_model import LogisticRegression # import logistic regression dari sklearn
logreg = LogisticRegression() # inisiasi object dengan nama logreg
logreg.fit(X_train, y_train) # fit model regression dari data train

# prediksi

y_pred = logreg.predict(X_test)
y_pred_train = logreg.predict(X_train)
y_pred_proba = logreg.predict_proba(X_test)
y_pred_proba_train = logreg.predict_proba(X_train)

"""**Model Evaluation: Pemilihan dan perhitungan metrics model**"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, y_pred))
print("Precision (Test Set): %.2f" % precision_score(y_test, y_pred))
print("Recall (Test Set): %.2f" % recall_score(y_test, y_pred))
print("F1-Score (Test Set): %.2f" % f1_score(y_test, y_pred))
    
print("roc_auc (test-proba): %.2f" % roc_auc_score(y_test, y_pred_proba[:, 1]))
print("roc_auc (train-proba): %.2f" % roc_auc_score(y_train, y_pred_proba_train[:, 1]))

"""**Model Evaluation: Apakah model sudah best-fit? Hindari Overfit/Underfit. Validasi dengan cross-validation**"""

from sklearn.model_selection import cross_validate

score = cross_validate(logreg,  X_train, y_train, cv=5, scoring='r2', return_train_score=True)
print('roc_auc (crossval train): '+ str(score['train_score'].mean()))
print('roc_auc (crossval test): '+ str(score['test_score'].mean()))

"""roc antara data train dan test tidak jauh. model tidak overfit

**Hyperparameter Tuning**
"""

from sklearn.model_selection import RandomizedSearchCV

penalty = ['l1', 'l2']
C = [float(x) for x in np.linspace(0.0001, 0.05, 100)]
hyperparameters = dict(penalty=penalty, C=C)

logreg = LogisticRegression()
rs = RandomizedSearchCV(logreg, hyperparameters, scoring='roc_auc', random_state=1, cv=5, n_iter=50)
rs.fit(X_train, y_train) 

# prediksi
y_pred = rs.predict(X_test)
y_pred_train = rs.predict(X_train)
y_pred_proba = rs.predict_proba(X_test)
y_pred_proba_train = rs.predict_proba(X_train)

print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, y_pred))
print("Precision (Test Set): %.2f" % precision_score(y_test, y_pred))
print("Recall (Test Set): %.2f" % recall_score(y_test, y_pred))
print("F1-Score (Test Set): %.2f" % f1_score(y_test, y_pred))
    
print("roc_auc (test-proba): %.2f" % roc_auc_score(y_test, y_pred_proba[:, 1]))
print("roc_auc (train-proba): %.2f" % roc_auc_score(y_train, y_pred_proba_train[:, 1]))

score = cross_validate(rs,  X_train, y_train, cv=5, scoring='r2', return_train_score=True)
print('roc_auc (crossval train): '+ str(score['train_score'].mean()))
print('roc_auc (crossval test): '+ str(score['test_score'].mean()))

"""**Check the best hyperparameter after tuning**"""

show_best_hyperparameter(rs)

"""# Feature Importance"""

X.columns

rs.best_estimator_.coef_

"""**Learning Curve**"""

import numpy as np
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

def draw_learning_curve(param_values):
    train_scores = []
    test_scores = []

    for c in param_values:
        model = LogisticRegression(penalty='l2', C=c)
        model.fit(X_train, y_train)

        # eval on train
        y_pred_train_proba = model.predict_proba(X_train)
        train_auc = roc_auc_score(y_train, y_pred_train_proba[:,1])
        train_scores.append(train_auc)

        # eval on test
        y_pred_proba = model.predict_proba(X_test)
        test_auc = roc_auc_score(y_test, y_pred_proba[:,1])
        test_scores.append(test_auc)

        print('param value: ' + str(c) + '; train: ' + str(train_auc) + '; test: '+ str(test_auc))

    plt.plot(param_values, train_scores, label='Train')
    plt.plot(param_values, test_scores, label='Test')
    plt.xlabel('C')
    plt.ylabel('AUC')
    plt.title('Learning Curve - Hyperparameter C - Logistic Regression')
    plt.legend()
    plt.show()

# learning curve
import numpy as np
param_values = [float(x) for x in np.linspace(0.0001, 0.05, 100)]
draw_learning_curve(param_values)

"""# Decision tree




"""

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

# decision tree
from sklearn.tree import DecisionTreeClassifier # import decision tree dari sklearn
dt = DecisionTreeClassifier() # inisiasi object dengan nama dt
dt.fit(X_train, y_train) # fit model decision tree dari data train

dt.feature_importances_

show_feature_importance(dt)